{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture 23 - Data Analysis via Least-Squares Fitting \n",
    " \n",
    "A ubiquitous task in engineering and other technical disciplines is the analysis of data.  Data comes from many sources, including measurements in the laboratory and numerical simulations.  In this lecture and the next, we'll apply what we've learned so far to produce *models* from data that we can use to explore trends and make conclusions.  Specifically, in this lecture, we'll develop models based on **least-squares** fitting of model parameters to existing data.  Such fitting may be the right choice when the data in contaminated by noise or when the model need only capture the main features of the observed phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to\n",
    "\n",
    "- Explain (and demonstrate) what is meant by a least-squares fit of a linear model $ax+b$ to a set of measured points $(x_i, y_i),\\, i = 0, 1, \\ldots$.\n",
    "- Use built-in tools to perform linear, least-squares fitting of data to models\n",
    "- Use built-in tools to perform nonlinear, least-squares fitting of data to models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Terms\n",
    "\n",
    "- error\n",
    "- least-squares\n",
    "- normal equation\n",
    "- `np.linalg.solve`\n",
    "- `np.polyfit`\n",
    "- `np.polyval`\n",
    "- `scipy.optimize.curve_fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Noisy Data\n",
    "\n",
    "A familiar task: draw a line of \"best fit\" through noisy data $(x_i, y_i)$\n",
    "\n",
    "Alternative description: find $a$ and $b$ so that the *linear model* $y(x) = ax + b$ best matches the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.sort(np.random.rand(50)*10)\n",
    "y = 0.25*t**2 + 3*t + 2 + np.random.normal(size=50)*t/2\n",
    "b = 0\n",
    "# connect first and last points\n",
    "a = (y[-1]-y[0])/(t[-1]-t[0])\n",
    "b = y[0] - a * t[0]\n",
    "y_a1 = a*t + b\n",
    "# connect minimum and maximum points\n",
    "t_max = t[np.where(y==max(y))] # np.where(y==max(y)) is one way \n",
    "t_min = t[np.where(y==min(y))] # to find the location of the max\n",
    "a = (max(y)-min(y))/(t_max-t_min)\n",
    "b = min(y) - a * t_min\n",
    "y_a2 = a*t + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(t, y, 'k.', t, y_a1, 'r--', t, y_a2, 'b-.')\n",
    "plt.legend([\"$y(t)$\", \"approx 1\", \"approx 2\"])\n",
    "plt.xlabel('t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What \"Best\" Means\n",
    "\n",
    "The error is the difference between the model and data:\n",
    "\n",
    "$$\n",
    "  e_i = y(t_i) - y_i = a t_i + b - y_i \\, \\quad i = 0, 1, \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A model is the **least-squares, best fit** if it minimizes \n",
    "\n",
    "$$\n",
    "  || \\mathbf{e} ||_2 = \\sqrt{ |e_0|^2 + |e_1|^2 + \\ldots} = \\sqrt{\\sum_{i=0} |e_i|^2} \\,  ,\n",
    "$$\n",
    "\n",
    "the $L_2$ norm (or euclidean norm) of the error.  Similar to other error metrics we've seen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**: Compute and plot the errors from example, and compute their L2-norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "e_1 =\n",
    "e_2 = \n",
    "norm_e_1 =\n",
    "norm_e_2 =\n",
    "plt.plot(t, e_1, 'r--', t, e_2, 'b-.')\n",
    "plt.legend([r\"$||e_1||_2$ = {:.4f}\".format(norm_e_1), \n",
    "            r\"$||e_2||_2$ = {:.4f}\".format(norm_e_2)])\n",
    "plt.xlabel('t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear least-square solutions are found by solving the normal equations:\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = (\\mathbf{M}^T \\mathbf{M})^{-1}  \\mathbf{M}^T \\mathbf{y} \\, .\n",
    "$$\n",
    "\n",
    "where the model attempts to satisfy $\\mathbf{Mc} = \\mathbf{y}$.  \n",
    "\n",
    "For $y(x) \\approx at + b$,  $\\mathbf{M} = [\\mathbf{x}, \\mathbf{1}]$ and $\\mathbf{c} = [a, b]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Determine the best-fit parameters $a$ and $b$ for the example data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Linear Models\n",
    "\n",
    "Not all data is intrinsically linear.  Alternatives:\n",
    "\n",
    " - polynomial models $y(x) = a_n x^n + a_{n-1} x^{n-1} +  \\ldots + a_0$.\n",
    " - problem-specific models, e.g., $y(x) = a_0 e^x + a_1 e^{-x} + a_2$.\n",
    " \n",
    "The key is that the coefficients $a_n$ are weights of separate, nonparameterized functions of $x$.  *Linear* least-squares fitting can be used (just like for $y(x) = ax+b$).  For a quadratic fit $y(x)=ax^2+bx+c$, $\\mathbf{M}$ becomes $[\\mathbf{x}^2, \\mathbf{x}^1, \\mathbf{x}^0]$ and $\\mathbf{c}$ becomes $\\mathbf{c} = [a, b, c]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For such polynomial fits, use `np.polyfit` and `np.polyval`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**:  Use `np.polyfit` and `np.polyval` to approximate $\\sin^2(x)$ as a linear, quadratic, and cubic polynomial for $x\\in [0, 2]$, and plot the approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)\n",
    "y = np.sin(x)**2\n",
    "# compute the approximations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'k', x, y_lin, 'b-.', \n",
    "         x, y_quad, 'g--', x, y_cub, 'r:')\n",
    "plt.legend(['y', 'linear', 'quadratic', 'cubic'])\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Polynomial Models\n",
    "\n",
    "Linear models are common, and polynomial models can provide necessary improvements.  For many cases, neither type of model is sufficient.  \n",
    "\n",
    "Consider data that needs to be fitted to \n",
    "\n",
    "$$\n",
    "    y(t) = a\\sin(bt + c) + d\n",
    "$$\n",
    "\n",
    "Because $b$ and $c$ are *inside* a nonlinear function, *linear* least-squares fitting is not applicable.  Rather, we need *nonlinear* least squares via, e.g., `scipy.optimize.curve_fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**:  A recent experiment led to the measurement of reactor powers as a function of time as tabulated below:\n",
    "\n",
    "| t | P |\n",
    "|---|---|\n",
    "| 0.0 | 2.945 |\n",
    "| 1.0 | 3.964 |\n",
    "| 2.0 | 4.481 |\n",
    "| 3.0 | 5.747 |\n",
    "| 4.0 | 7.523 |\n",
    "| 5.0 | 8.71 |\n",
    "| 6.0 | 10.733 |\n",
    "| 7.0 | 13.91 |\n",
    "| 8.0 | 16.721 |\n",
    "| 9.0 | 19.951 |\n",
    "| 10.0 | 24.61 |\n",
    "\n",
    "Fit this data to the model $P(t) = ae^{bt}$.  Is there a way we could use regular old linear regression effectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "You should now be able to:\n",
    "\n",
    "- Explain (and demonstrate) what is meant by a least-squares fit of a linear model $ax+b$ to a set of measured points $(x_i, y_i),\\, i = 0, 1, \\ldots$.\n",
    "- Use built-in tools to perform linear, least-squares fitting of data to models\n",
    "- Use built-in tools to perform nonlinear, least-squares fitting of data to models"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "livereveal": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
